# train_dqn.py
# (ê²½ë¡œ: /Users/liam/Desktop/Othello/train_dqn.py)

import torch
import numpy as np
import os
import matplotlib.pyplot as plt
from othello_env_agent import OthelloEnv, DQNAgent 

# ============================================================
# ë¡œì»¬ í™˜ê²½ ì„¤ì •
# ============================================================
PROJECT_DIR = '/Users/liam/Desktop/Othello'
MODEL_SAVE_PATH = os.path.join(PROJECT_DIR, "dqn_othello_model.pth")
os.makedirs(PROJECT_DIR, exist_ok=True) 

# ============================================================
# 4ï¸âƒ£ DQN í•™ìŠµ ë£¨í”„ (train_agent í•¨ìˆ˜ ì •ì˜)
# ============================================================
def train_agent(agent, episodes=50000, target_update_freq=100):
    env = agent.env
    rewards = []
    losses = []

    print(f"--- {agent.__class__.__name__} í•™ìŠµ ì‹œì‘ (Episodes: {episodes}, LR: {agent.optimizer.param_groups[0]['lr']}) ---")

    for ep in range(episodes):
        # gymnasium resetì€ (state, info) íŠœí”Œ ë°˜í™˜
        state, _ = env.reset() 
        terminated = False
        truncated = False
        total_reward = 0
        step = 0

        # â­â­ ë£¨í”„ ì¡°ê±´: terminated(ìŠ¹íŒ¨) ë˜ëŠ” truncated(íƒ€ì„ì•„ì›ƒ)ê°€ ì•„ë‹ ë•Œê¹Œì§€ â­â­
        while not (terminated or truncated): 
            
            valid_moves = env.valid_moves()
            
            if not valid_moves and env.game_over:
                break

            action = agent.act(state, valid_moves)
            if action is None:
                break

            # â­â­â­ step() ë°˜í™˜ ê°’ ìˆ˜ì •: 5ê°œ(next_state, reward, terminated, truncated, info) ë°›ê¸° â­â­â­
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            # DQNAgent.rememberëŠ” doneì„ ì‚¬ìš©í•˜ë¯€ë¡œ terminatedë¥¼ doneìœ¼ë¡œ ì‚¬ìš©
            done = terminated 
            
            total_reward += reward

            # DQN ì—…ë°ì´íŠ¸
            agent.remember(state, action, reward, next_state, done)
            loss = agent.replay() 
            if loss is not None:
                losses.append(loss)

            state = next_state
            step += 1

            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if step % target_update_freq == 0:
                agent.update_target()
        
        agent.decay_epsilon()
        rewards.append(total_reward)

        if (ep + 1) % 100 == 0:
            avg_reward_100 = np.mean(rewards[-100:])
            avg_loss = np.mean(losses[-100:]) if losses else 0
            
            print(f"Ep {ep+1}/{episodes}, Avg Reward (100 Eps)={avg_reward_100:.2f}, Avg Loss={avg_loss:.4f}, Epsilon={agent.epsilon:.5f}")

    # ì‹œê°í™”: ë³´ìƒ ê³¡ì„ 
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(rewards, label='Total Reward')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title(f'{agent.__class__.__name__} Training Reward Curve')
    plt.legend()
    
    # ì‹œê°í™”: Loss ê³¡ì„ 
    plt.subplot(1, 2, 2)
    plt.plot(losses, label='Loss', color='red')
    plt.xlabel('Replay Step')
    plt.ylabel('MSE Loss')
    plt.title(f'{agent.__class__.__name__} Training Loss')
    plt.legend()
    plt.show()

    return rewards, losses


# ============================================================
# 5ï¸âƒ£ ì‹¤í–‰ ë° ëª¨ë¸ ì €ì¥
# ============================================================
if __name__ == '__main__':
    env = OthelloEnv()
    
    # DQN Agent ìƒì„± (M1/MPSëŠ” othello_env_agent.pyì—ì„œ ì„¤ì •ë¨)
    # LR=1e-4, Batch Size=128 ì ìš©
    dqn_agent = DQNAgent(env,
                         gamma=0.95,
                         lr=1e-4, 
                         epsilon_decay=0.9999,
                         epsilon_min=0.1,
                         batch_size=128) # Batch Size 128 ì ìš©

    N_EPISODES = 50000
    print(f"ğŸ§  CNN ê¸°ë°˜ DQN Agent í•™ìŠµ ì‹œì‘! (Episodes: {N_EPISODES}, Batch Size: {dqn_agent.batch_size}, LR: {dqn_agent.optimizer.param_groups[0]['lr']})")
    
    train_agent(dqn_agent, episodes=N_EPISODES) 

    print(f"\nâœ… DQN Agent í•™ìŠµ ì™„ë£Œ. ëª¨ë¸ ì €ì¥ ì¤‘... ê²½ë¡œ: {MODEL_SAVE_PATH}")
    try:
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
        torch.save(dqn_agent.q_net.state_dict(), MODEL_SAVE_PATH)
        print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: ê²½ë¡œ í™•ì¸ í•„ìš”. ì˜¤ë¥˜: {e}")
