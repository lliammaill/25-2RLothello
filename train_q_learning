# train_q_learning.py
# (ê²½ë¡œ: /Users/liam/Desktop/Othello/train_q_learning.py)

import pickle
import numpy as np
import os
import matplotlib.pyplot as plt
from othello_env_agent import OthelloEnv, QLearningAgent # othello_env_agent.py íŒŒì¼ì—ì„œ ì§ì ‘ import


# ============================================================
# ë¡œì»¬ í™˜ê²½ ì„¤ì • (ê²½ë¡œ ë³€ê²½)
# ============================================================
PROJECT_DIR = '/Users/liam/Desktop/Othello' 
Q_TABLE_SAVE_PATH = os.path.join(PROJECT_DIR, "q_table_othello.pkl")
os.makedirs(PROJECT_DIR, exist_ok=True) 


# ============================================================
# 4ï¸âƒ£ Q-Learning í•™ìŠµ ë£¨í”„ (train_agent í•¨ìˆ˜ ì •ì˜)
# ============================================================
def train_agent(agent, episodes=200000):
    env = agent.env
    rewards = []

    print(f"--- {agent.__class__.__name__} í•™ìŠµ ì‹œì‘ (Episodes: {episodes}) ---")

    for ep in range(episodes):
        state, _ = env.reset() # gymnasium resetì€ (state, info) íŠœí”Œ ë°˜í™˜
        
        # gymì˜ done ëŒ€ì‹ , gymnasiumì˜ terminatedì™€ truncated ì‚¬ìš©
        terminated = False
        truncated = False
        total_reward = 0

        # â­â­ ë£¨í”„ ì¡°ê±´ ìˆ˜ì •: terminated(ìŠ¹íŒ¨) ë˜ëŠ” truncated(íƒ€ì„ì•„ì›ƒ)ê°€ ì•„ë‹ ë•Œê¹Œì§€ â­â­
        while not (terminated or truncated): 
            
            valid_moves = env.valid_moves()
            
            # ì˜¤ë¸ë¡œì˜ game_over ìƒíƒœëŠ” env.step()ì—ì„œ terminated=Trueë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ,
            # ì´ ë¡œì§ì€ ê²Œì„ì´ ì •ì§€ëœ ìƒíƒœ(ë‘˜ ê³³ì´ ì—†ìœ¼ë‚˜ ì¢…ë£ŒëŠ” ì•„ë‹Œ)ë¥¼ ìœ„í•œ ë°©ì–´ ë¡œì§ìœ¼ë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
            if not valid_moves and env.game_over: 
                break

            action = agent.act(state, valid_moves)
            if action is None:
                break

            # â­â­â­ step() ë°˜í™˜ ê°’ ìˆ˜ì •: 5ê°œ(next_state, reward, terminated, truncated, info) ë°›ê¸° â­â­â­
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            # QLearningAgent.rememberëŠ” doneì„ ì‚¬ìš©í•˜ë¯€ë¡œ terminatedë¥¼ doneìœ¼ë¡œ ì‚¬ìš©
            done = terminated 
            
            total_reward += reward

            # Q-Learning ì—…ë°ì´íŠ¸
            agent.remember(state, action, reward, next_state, done)
            agent.replay() 

            state = next_state

        agent.decay_epsilon()

        rewards.append(total_reward)

        if (ep+1) % 1000 == 0:
            avg_reward_1000 = np.mean(rewards[-1000:])
            print(f"Ep {ep+1}/{episodes}, Avg Reward (1000 Eps)={avg_reward_1000:.2f}, Epsilon={agent.epsilon:.5f}, Q-Table Size={len(agent.q_table)}")

    # ì‹œê°í™”
    plt.figure(figsize=(8,5))
    plt.plot(rewards, label='Total Reward')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title(f'{agent.__class__.__name__} Training Reward Curve')
    plt.legend()
    plt.show()

    return rewards


# ============================================================
# 5ï¸âƒ£ ì‹¤í–‰ ë° Q-Table ì €ì¥
# ============================================================
if __name__ == '__main__':
    env = OthelloEnv()

    # Q-Learning Agent ìƒì„± (5000 ì—í”¼ì†Œë“œì— ë§ì¶° ê°ì‡ ìœ¨ ì¡°ì •)
    q_agent = QLearningAgent(env,
                             alpha=0.1,
                             gamma=0.95,
                             epsilon_decay=0.9999, 
                             epsilon_min=0.01)

    N_EPISODES = 200000
    print(f"ğŸ§  Q-Learning Agent í•™ìŠµ ì‹œì‘! (Episodes: {N_EPISODES})")
    train_agent(q_agent, episodes=N_EPISODES) 

    print(f"\nâœ… Q-Learning Agent í•™ìŠµ ì™„ë£Œ. Q-Table ì €ì¥ ì¤‘... ê²½ë¡œ: {Q_TABLE_SAVE_PATH}")
    try:
        # defaultdictë¥¼ ìˆœìˆ˜ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        q_table_data = dict(q_agent.q_table) 
        
        with open(Q_TABLE_SAVE_PATH, 'wb') as f:
            pickle.dump(q_table_data, f)
            
        print(f"Q-Table ì €ì¥ ì™„ë£Œ! (í¬ê¸°: {len(q_table_data)})")
    except Exception as e:
        print(f"Q-Table ì €ì¥ ì‹¤íŒ¨: ê²½ë¡œ í™•ì¸ í•„ìš”. ì˜¤ë¥˜: {e}")
