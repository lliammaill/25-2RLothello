
# ============================================================
# 1ï¸âƒ£ í™˜ê²½ ì •ì˜ (Gym ìŠ¤íƒ€ì¼)
# ============================================================
import numpy as np
import gym
from gym import spaces

class OthelloEnv(gym.Env):
    def __init__(self):
        super().__init__()
        self.size = 8
        self.action_space = spaces.Discrete(self.size * self.size)
        self.observation_space = spaces.Box(low=0, high=2, shape=(self.size, self.size), dtype=int)
        self.reset()

    def reset(self):
        self.board = np.zeros((self.size, self.size), dtype=int)
        mid = self.size // 2
        self.board[mid-1][mid-1], self.board[mid][mid] = 2, 2
        self.board[mid-1][mid], self.board[mid][mid-1] = 1, 1
        self.current_player = 1
        return self.board.copy()

    def get_opponent(self):
        return 3 - self.current_player

    def is_on_board(self, x, y):
        return 0 <= x < self.size and 0 <= y < self.size

    def valid_moves(self):
        directions = [(-1,-1), (-1,0), (-1,1),
                      (0,-1),          (0,1),
                      (1,-1),  (1,0),  (1,1)]
        valid = set()
        opponent = self.get_opponent()
        for x in range(self.size):
            for y in range(self.size):
                if self.board[x,y] != 0:
                    continue
                for dx, dy in directions:
                    nx, ny = x + dx, y + dy
                    if self.is_on_board(nx, ny) and self.board[nx, ny] == opponent:
                        while self.is_on_board(nx, ny):
                            if self.board[nx, ny] == 0:
                                break
                            if self.board[nx, ny] == self.current_player:
                                valid.add(x*self.size + y)
                                break
                            nx += dx
                            ny += dy
        return valid

    def flip_discs(self, x, y):
        directions = [(-1,-1), (-1,0), (-1,1),
                      (0,-1),          (0,1),
                      (1,-1),  (1,0),  (1,1)]
        opponent = self.get_opponent()
        to_flip = []
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            temp = []
            while self.is_on_board(nx, ny) and self.board[nx, ny] == opponent:
                temp.append((nx, ny))
                nx += dx
                ny += dy
            if self.is_on_board(nx, ny) and self.board[nx, ny] == self.current_player:
                to_flip.extend(temp)
        for fx, fy in to_flip:
            self.board[fx, fy] = self.current_player
        return len(to_flip)

    def step(self, action):
        x, y = divmod(action, self.size)
        valid = self.valid_moves()
        if action not in valid:
            reward = -1
            done = False
            info = {"invalid_move": True}
            return self.board.copy(), reward, done, info

        self.board[x, y] = self.current_player
        flipped = self.flip_discs(x, y)
        reward = flipped

        # âœ… ì½”ë„ˆ ë³´ìƒ ì¶”ê°€ (ìµœì™¸ê³½ 4ê°œ ì½”ë„ˆ ì¹¸)
        corners = [(0,0), (0,self.size-1), (self.size-1,0), (self.size-1, self.size-1)]
        if (x, y) in corners:
                reward += 10

        # âœ… ìŠ¹íŒ¨ ë³´ìƒ ì¶”ê°€
        if np.all(self.board != 0) or not self.valid_moves():
            black, white = np.sum(self.board == 1), np.sum(self.board == 2)
            if black > white and self.current_player == 2:
                reward += 200
            elif white > black and self.current_player == 1:
                reward -= 200
            done = True
        else:
            done = False

        self.current_player = self.get_opponent()
        if not self.valid_moves():
            self.current_player = self.get_opponent()
            done = not self.valid_moves()

        info = {"flipped": flipped}
        return self.board.copy(), reward, done, info

    def render(self):
        print(self.board)


# ============================================================
# 2ï¸âƒ£ Q-learning Agent
# ============================================================
import random
from collections import defaultdict

class QLearningAgent:
    def __init__(self, env, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1):
        self.env = env
        self.q_table = defaultdict(lambda: np.zeros(env.action_space.n))
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

    def state_to_key(self, state):
        return tuple(state.flatten())

    def choose_action(self, state, valid_moves):
        state_key = self.state_to_key(state)
        if not valid_moves:
            return None
        if random.random() < self.epsilon:
            return random.choice(list(valid_moves))
        q_values = self.q_table[state_key]
        q_values_filtered = [q_values[a] if a in valid_moves else -np.inf for a in range(self.env.action_space.n)]
        return int(np.argmax(q_values_filtered))

    def learn(self, state, action, reward, next_state, done):
        state_key = self.state_to_key(state)
        next_key = self.state_to_key(next_state)
        q_predict = self.q_table[state_key][action]
        q_target = reward + (0 if done else self.gamma * np.max(self.q_table[next_key]))
        self.q_table[state_key][action] += self.alpha * (q_target - q_predict)

    def decay_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)


# ============================================================
# 3ï¸âƒ£ CNN ê¸°ë°˜ DQN Agent
# ============================================================
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

class DQNetwork(nn.Module):
    def __init__(self, input_channels=1, output_dim=64):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU()
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim)
        )

    def forward(self, x):
        return self.fc(self.conv(x))

class DQNAgent:
    def __init__(self, env, gamma=0.95, lr=1e-3, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.1, buffer_size=50000, batch_size=64):
        self.env = env
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size

        self.memory = deque(maxlen=buffer_size)
        self.q_net = DQNetwork()
        self.target_net = DQNetwork()
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()
        self.update_target()

    def update_target(self):
        self.target_net.load_state_dict(self.q_net.state_dict())

    def act(self, state, valid_moves):
        if not valid_moves:
            return None
        if np.random.rand() < self.epsilon:
            return random.choice(list(valid_moves))
        state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)
        q_values = self.q_net(state_tensor).detach().numpy()[0]
        q_filtered = [q_values[a] if a in valid_moves else -np.inf for a in range(self.env.action_space.n)]
        return int(np.argmax(q_filtered))

    def remember(self, s, a, r, s2, done):
        self.memory.append((s, a, r, s2, done))

    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        s, a, r, s2, done = zip(*batch)
        s = torch.FloatTensor(np.array(s)).unsqueeze(1)
        s2 = torch.FloatTensor(np.array(s2)).unsqueeze(1)
        r = torch.FloatTensor(r)
        a = torch.LongTensor(a)
        done = torch.FloatTensor(done)

        q_values = self.q_net(s).gather(1, a.unsqueeze(1)).squeeze()
        next_q = self.target_net(s2).max(1)[0]
        target = r + (1 - done) * self.gamma * next_q

        loss = self.loss_fn(q_values, target.detach())
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def decay_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

# ============================================================
# 4ï¸âƒ£ ê³µí†µ í•™ìŠµ ë£¨í”„ (with reward tracking)
# ============================================================
import matplotlib.pyplot as plt

def train_agent(agent, episodes=1000):
    env = agent.env
    rewards = []

    for ep in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0

        while not done:
            valid_moves = env.valid_moves()
            if not valid_moves:
                break
            if isinstance(agent, QLearningAgent):
                action = agent.choose_action(state, valid_moves)
            else:
                action = agent.act(state, valid_moves)

            next_state, reward, done, _ = env.step(action)
            total_reward += reward

            if isinstance(agent, QLearningAgent):
                agent.learn(state, action, reward, next_state, done)
            else:
                agent.remember(state, action, reward, next_state, done)
                agent.replay()

            state = next_state

        agent.decay_epsilon()
        if hasattr(agent, 'update_target') and (ep+1) % 50 == 0:
            agent.update_target()
        rewards.append(total_reward)

        if (ep+1) % 50 == 0:
            print(f"{agent.__class__.__name__} Episode {ep+1}, total_reward={total_reward:.2f}, epsilon={agent.epsilon:.3f}")

    # âœ… í•™ìŠµ ê³¡ì„  ì‹œê°í™”
    plt.figure(figsize=(8,4))
    plt.plot(rewards, label=f'{agent.__class__.__name__} Reward')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.title(f'Training Reward Curve - {agent.__class__.__name__}')
    plt.legend()
    plt.show()

    return rewards

# ============================================================
# 5ï¸âƒ£ í‘/ë°± êµëŒ€ ëŒ€ê²° í•¨ìˆ˜
# ============================================================
def duel_with_color_swap(env, q_agent, dqn_agent, episodes=30):
    def play_match(env, black_agent, white_agent):
        state = env.reset()
        done = False
        env.current_player = 1

        while not done:
            valid_moves = env.valid_moves()
            if not valid_moves:
                env.current_player = env.get_opponent()
                if not env.valid_moves():
                    break
                continue

            current_agent = black_agent if env.current_player == 1 else white_agent
            if isinstance(current_agent, QLearningAgent):
                action = current_agent.choose_action(state, valid_moves)
            else:
                action = current_agent.act(state, valid_moves)

            state, _, done, _ = env.step(action)

        black = np.sum(env.board == 1)
        white = np.sum(env.board == 2)
        return black, white

    q_wins = dqn_wins = draws = 0
    for i in range(episodes):
        b, w = play_match(env, q_agent, dqn_agent)
        if b > w: q_wins += 1
        elif w > b: dqn_wins += 1
        else: draws += 1
    for i in range(episodes):
        b, w = play_match(env, dqn_agent, q_agent)
        if b > w: dqn_wins += 1
        elif w > b: q_wins += 1
        else: draws += 1

    print(f"\nâš”ï¸ ìƒ‰ìƒ êµëŒ€ ëŒ€ê²° ê²°ê³¼ (ì´ {episodes*2}íŒ)")
    print(f"Q-learning ìŠ¹: {q_wins}, DQN ìŠ¹: {dqn_wins}, ë¬´ìŠ¹ë¶€: {draws}")
    print(f"DQN ì¢…í•© ìŠ¹ë¥ : {dqn_wins / (episodes*2):.2%} (ìƒ‰ìƒ ë¶ˆë¬¸)")

# ============================================================
# 6ï¸âƒ£ ì‹¤í–‰
# ============================================================
env = OthelloEnv()

print("ğŸ§© Q-learning Agent í•™ìŠµ ì¤‘...")
q_agent = QLearningAgent(env)
train_agent(q_agent, episodes=1000)

print("\nğŸ§  CNN ê¸°ë°˜ DQN Agent í•™ìŠµ ì¤‘...")
dqn_agent = DQNAgent(env)
train_agent(dqn_agent, episodes=1000)

print("\nâš”ï¸ í‘/ë°± êµëŒ€ ëŒ€ê²° ê²°ê³¼:")
duel_with_color_swap(env, q_agent, dqn_agent, episodes=30)


